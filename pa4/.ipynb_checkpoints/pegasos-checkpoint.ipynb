{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "azpSBQVLwqCt"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "from scipy.linalg.blas import ddot\n",
    "from scipy.linalg.blas import dscal\n",
    "from scipy.linalg.blas import daxpy\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class LinearClassifier(BaseEstimator):\n",
    "    \"\"\"\n",
    "    General class for binary linear classifiers. Implements the predict\n",
    "    function, which is the same for all binary linear classifiers. There are\n",
    "    also two utility functions.\n",
    "    \"\"\"\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Computes the decision function for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "        return X.dot(self.w)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the outputs for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "\n",
    "        # First compute the output scores\n",
    "        scores = self.decision_function(X)\n",
    "\n",
    "        # Select the positive or negative class label, depending on whether\n",
    "        # the score was positive or negative.\n",
    "        out = np.select([scores >= 0.0, scores < 0.0],\n",
    "                        [self.positive_class,\n",
    "                         self.negative_class])\n",
    "        return out\n",
    "\n",
    "    def find_classes(self, Y):\n",
    "        \"\"\"\n",
    "        Finds the set of output classes in the output part Y of the training set.\n",
    "        If there are exactly two classes, one of them is associated to positive\n",
    "        classifier scores, the other one to negative scores. If the number of\n",
    "        classes is not 2, an error is raised.\n",
    "        \"\"\"\n",
    "        classes = sorted(set(Y))\n",
    "        if len(classes) != 2:\n",
    "            raise Exception(\"this does not seem to be a 2-class problem\")\n",
    "        self.positive_class = classes[1]\n",
    "        self.negative_class = classes[0]\n",
    "\n",
    "    def encode_outputs(self, Y):\n",
    "        \"\"\"\n",
    "        A helper function that converts all outputs to +1 or -1.\n",
    "        \"\"\"\n",
    "        return np.array([1 if y == self.positive_class else -1 for y in Y])\n",
    "\n",
    "\n",
    "class Pegasos(LinearClassifier):\n",
    "    \"\"\"\n",
    "    Implementation of the pegasos learning algorithm using hinge-loss function.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the pegasos learning algorithm with hinge-loss.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        \n",
    "        # Pegasos algorithm\n",
    "        \n",
    "        # Number of pairs to be randomly selected\n",
    "        T = 100000\n",
    "        \n",
    "        # Lambda\n",
    "        Lambda = 1/T\n",
    "        \n",
    "        for t in range(1,T+1):\n",
    "            i = np.random.randint(1,len(X))\n",
    "            \n",
    "            #learning rate\n",
    "            nu = 1/(Lambda*t)\n",
    "\n",
    "            x = X[i]\n",
    "            y = Ye[i]\n",
    "            score = np.dot(self.w,x)\n",
    "            \n",
    "            if y*score < 1:\n",
    "                self.w = (1-nu*Lambda)*self.w + (nu*y)*x\n",
    "            else:\n",
    "                self.w = (1-nu*Lambda)*self.w\n",
    "                \n",
    "                \n",
    "class Pegasos_BLAS(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the pegasus learning algorithm using BLAS-functions\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the pegasos learning algorithm using BLAS-functions\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        \n",
    "        # Number of pairs to be randomly selected\n",
    "        T = 100000\n",
    "        \n",
    "        # Lambda\n",
    "        Lambda = 1/T\n",
    "                \n",
    "        ### Pegasos algorithm using BLAS functions\n",
    "        \n",
    "        for t in range(1,T+1):\n",
    "            i = np.random.randint(1,len(X))\n",
    "            \n",
    "            #learning rate\n",
    "            nu = 1/(Lambda*t)\n",
    "\n",
    "            x = X[i]\n",
    "            y = Ye[i]\n",
    "            score = ddot(self.w,x)\n",
    "            \n",
    "            if y*score < 1:\n",
    "                #dscal(1-nu*Lambda,self.w)\n",
    "                #daxpy(x,self.w,a = ddot(nu,y))\n",
    "                daxpy(x,dscal(1-ddot(nu,Lambda),self.w),a=ddot(nu,y))\n",
    "                                           \n",
    "            else:           \n",
    "                dscal(1-nu*Lambda,self.w)\n",
    "                        \n",
    "        \n",
    "     \n",
    "                       \n",
    "class Pegasos_LR(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the pegasos learning algorithm using log-loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the pegasos learning algorithm with log-loss.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        \n",
    "        # Pegasos algorithm\n",
    "        \n",
    "        # Number of pairs to be randomly selected\n",
    "        T = 100000\n",
    "        epochs = np.round(T/10)\n",
    "        \n",
    "        # Lambda\n",
    "        Lambda = 1/T\n",
    "        \n",
    "        sum_loss = 0\n",
    "        \n",
    "        for t in range(1,T+1):\n",
    "            i = np.random.randint(1,len(X))\n",
    "            #learning rate\n",
    "            nu = 1/(Lambda*t)\n",
    "\n",
    "            x = X[i]\n",
    "            y = Ye[i]\n",
    "            score = np.dot(self.w,x)\n",
    "            \n",
    "            loss = -(y*x)/(1+ np.exp(y*score))\n",
    "            \n",
    "            sum_loss = sum_loss + np.sum(loss)\n",
    "            \n",
    "            self.w = self.w - nu*(Lambda*self.w +loss)\n",
    "            \n",
    "            # printing current value of the objective function at each iteration.\n",
    "            if t == epochs:\n",
    "                epochs = epochs + 10000\n",
    "                \n",
    "                print(f'Objective function at {t}:{sum_loss/t + (Lambda/2)*((LA.norm(self.w))**2)}')\n",
    "                   \n",
    "                           \n",
    "            \n",
    "        \n",
    "\n",
    "##### The following part is for the optional task.\n",
    "\n",
    "### Sparse and dense vectors don't collaborate very well in NumPy/SciPy.\n",
    "### Here are two utility functions that help us carry out some vector\n",
    "### operations that we'll need.\n",
    "\n",
    "def add_sparse_to_dense(x, w, factor):\n",
    "    \"\"\"\n",
    "    Adds a sparse vector x, scaled by some factor, to a dense vector.\n",
    "    This can be seen as the equivalent of w += factor * x when x is a dense\n",
    "    vector.\n",
    "    \"\"\"\n",
    "    w[x.indices] += factor * x.data\n",
    "\n",
    "def sparse_dense_dot(x, w):\n",
    "    \"\"\"\n",
    "    Computes the dot product between a sparse vector x and a dense vector w.\n",
    "    \"\"\"\n",
    "    return np.dot(w[x.indices], x.data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SparsePegasos(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the pegasos learning algorithm,\n",
    "    assuming that the input feature matrix X is sparse.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the pegasos learning algorithm.\n",
    "\n",
    "        Note that this will only work if X is a sparse matrix, such as the\n",
    "        output of a scikit-learn vectorizer.\n",
    "        \"\"\"\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "\n",
    "        # Iteration through sparse matrices can be a bit slow, so we first\n",
    "        # prepare this list to speed up iteration.\n",
    "        XY = list(zip(X, Ye))\n",
    "        \n",
    "        # number of iterations\n",
    "        T = 100000\n",
    "        \n",
    "        # Lambda\n",
    "        Lambda = 1/T\n",
    "        \n",
    "        for t in range(1,T+1):\n",
    "            i = np.random.randint(1,len(XY))\n",
    "            \n",
    "            #learning rate\n",
    "            nu = 1/(Lambda*t)\n",
    "\n",
    "            x = XY[i][0]\n",
    "            y = XY[i][1]\n",
    "           \n",
    "            score = sparse_dense_dot(x,self.w)\n",
    "            \n",
    "            if y*score < 1:\n",
    "                self.w = (1-nu*Lambda)*self.w\n",
    "                add_sparse_to_dense(x,self.w,nu*y)\n",
    "            else:\n",
    "                self.w = (1-nu*Lambda)*self.w\n",
    "                \n",
    "\n",
    "class SparsePegasos_scale(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the pegasos learning algorithm,\n",
    "    assuming that the input feature matrix X is sparse. \n",
    "    \n",
    "    In this implementation we use a scaling trick to speed up the process.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=20):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "\n",
    "        Note that this will only work if X is a sparse matrix, such as the\n",
    "        output of a scikit-learn vectorizer.\n",
    "        \"\"\"\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "\n",
    "        # Iteration through sparse matrices can be a bit slow, so we first\n",
    "        # prepare this list to speed up iteration.\n",
    "        XY = list(zip(X, Ye))\n",
    "        \n",
    "        #print(len(XY)) \n",
    "        T = 100000\n",
    "        \n",
    "        # Lambda\n",
    "        Lambda = 1/T\n",
    "        \n",
    "        a = 1 \n",
    "        \n",
    "        for t in range(1,T+1):\n",
    "            i = np.random.randint(1,len(XY))\n",
    "            \n",
    "            #learning rate\n",
    "            nu = 1/(Lambda*t)\n",
    "\n",
    "            x = XY[i][0]\n",
    "            y = XY[i][1]\n",
    "            \n",
    "            a = (1-nu*Lambda)*a\n",
    "           \n",
    "            score = sparse_dense_dot(x,self.w)*a\n",
    "            \n",
    "            if y*score < 1:\n",
    "                \n",
    "                add_sparse_to_dense(x,self.w,(nu*y)/a)       \n",
    "                \n",
    "            else:\n",
    "                \n",
    "                self.w = self.w\n",
    "          \n",
    "        self.w = a*self.w\n",
    "                \n",
    "        \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "pegasos.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
