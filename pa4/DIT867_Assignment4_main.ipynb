{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f9bcd85",
   "metadata": {},
   "source": [
    "## DAT340/DIT867 Programming assignment 4: Implementing linear classifiers\n",
    "\n",
    "#### Calvin Smith\n",
    "#### Bragadesh Bharatwaj Sundararaman\n",
    "#### Amogha Udayakumar\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad27b7bd",
   "metadata": {},
   "source": [
    "### Exercise question:\n",
    "\n",
    "In the pipeline, DictVectorizer is used. It converts the string input into vectors which are then input to the perceptron. When we look closely at the output of the dictVectorizer we can find that in the first case the input is clearly linearly separable. Thus the perceptron is able to perfectly classify it. On the other hand, in the second training data, when we analyze the dictvectorizer output we can see that the data is not linearly separable. This is the reason why even if we utilize Linear SVC it still fails. In order to classify linearly inseparable data, we can either use a non-linear classifier or if using a linear classifier we have to increase the dimensions of the input i.e. add more features to the input by creating new features using existing ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68a827f",
   "metadata": {},
   "source": [
    "### Pegasos classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "283cf023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "from scipy.linalg.blas import ddot\n",
    "from scipy.linalg.blas import dscal\n",
    "from scipy.linalg.blas import daxpy\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class LinearClassifier(BaseEstimator):\n",
    "    \"\"\"\n",
    "    General class for binary linear classifiers. Implements the predict\n",
    "    function, which is the same for all binary linear classifiers. There are\n",
    "    also two utility functions.\n",
    "    \"\"\"\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Computes the decision function for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "        return X.dot(self.w)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the outputs for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "\n",
    "        # First compute the output scores\n",
    "        scores = self.decision_function(X)\n",
    "\n",
    "        # Select the positive or negative class label, depending on whether\n",
    "        # the score was positive or negative.\n",
    "        out = np.select([scores >= 0.0, scores < 0.0],\n",
    "                        [self.positive_class,\n",
    "                         self.negative_class])\n",
    "        return out\n",
    "\n",
    "    def find_classes(self, Y):\n",
    "        \"\"\"\n",
    "        Finds the set of output classes in the output part Y of the training set.\n",
    "        If there are exactly two classes, one of them is associated to positive\n",
    "        classifier scores, the other one to negative scores. If the number of\n",
    "        classes is not 2, an error is raised.\n",
    "        \"\"\"\n",
    "        classes = sorted(set(Y))\n",
    "        if len(classes) != 2:\n",
    "            raise Exception(\"this does not seem to be a 2-class problem\")\n",
    "        self.positive_class = classes[1]\n",
    "        self.negative_class = classes[0]\n",
    "\n",
    "    def encode_outputs(self, Y):\n",
    "        \"\"\"\n",
    "        A helper function that converts all outputs to +1 or -1.\n",
    "        \"\"\"\n",
    "        return np.array([1 if y == self.positive_class else -1 for y in Y])\n",
    "\n",
    "\n",
    "class Pegasos(LinearClassifier):\n",
    "    \"\"\"\n",
    "    Implementation of the pegasos learning algorithm using hinge-loss function.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the pegasos learning algorithm with hinge-loss.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        \n",
    "        # Pegasos algorithm\n",
    "        \n",
    "        # Number of pairs to be randomly selected\n",
    "        T = 100000\n",
    "        \n",
    "        # Lambda\n",
    "        Lambda = 1/T\n",
    "        \n",
    "        for t in range(1,T+1):\n",
    "            i = np.random.randint(1,len(X))\n",
    "            \n",
    "            #learning rate\n",
    "            nu = 1/(Lambda*t)\n",
    "\n",
    "            x = X[i]\n",
    "            y = Ye[i]\n",
    "            score = np.dot(self.w,x)\n",
    "            \n",
    "            if y*score < 1:\n",
    "                self.w = (1-nu*Lambda)*self.w + (nu*y)*x\n",
    "            else:\n",
    "                self.w = (1-nu*Lambda)*self.w\n",
    "                \n",
    "                \n",
    "class Pegasos_BLAS(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the pegasus learning algorithm using BLAS-functions\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the pegasos learning algorithm using BLAS-functions\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        \n",
    "        # Number of pairs to be randomly selected\n",
    "        T = 100000\n",
    "        \n",
    "        # Lambda\n",
    "        Lambda = 1/T\n",
    "                \n",
    "        ### Pegasos algorithm using BLAS functions\n",
    "        \n",
    "        for t in range(1,T+1):\n",
    "            i = np.random.randint(1,len(X))\n",
    "            \n",
    "            #learning rate\n",
    "            nu = 1/(Lambda*t)\n",
    "\n",
    "            x = X[i]\n",
    "            y = Ye[i]\n",
    "            score = ddot(self.w,x)\n",
    "            \n",
    "            if y*score < 1:\n",
    "                #dscal(1-nu*Lambda,self.w)\n",
    "                #daxpy(x,self.w,a = ddot(nu,y))\n",
    "                daxpy(x,dscal(1-ddot(nu,Lambda),self.w),a=ddot(nu,y))\n",
    "                                           \n",
    "            else:           \n",
    "                dscal(1-nu*Lambda,self.w)\n",
    "                        \n",
    "        \n",
    "     \n",
    "                       \n",
    "class Pegasos_LR(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the pegasos learning algorithm using log-loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the pegasos learning algorithm with log-loss.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        \n",
    "        # Pegasos algorithm\n",
    "        \n",
    "        # Number of pairs to be randomly selected\n",
    "        T = 100000\n",
    "        epochs = np.round(T/10)\n",
    "        \n",
    "        # Lambda\n",
    "        Lambda = 1/T\n",
    "        \n",
    "        sum_loss = 0\n",
    "        \n",
    "        for t in range(1,T+1):\n",
    "            i = np.random.randint(1,len(X))\n",
    "            #learning rate\n",
    "            nu = 1/(Lambda*t)\n",
    "\n",
    "            x = X[i]\n",
    "            y = Ye[i]\n",
    "            score = np.dot(self.w,x)\n",
    "            \n",
    "            loss = -(y*x)/(1+ np.exp(y*score))\n",
    "            \n",
    "            sum_loss = sum_loss + np.sum(loss)\n",
    "            \n",
    "            self.w = self.w - nu*(Lambda*self.w +loss)\n",
    "            \n",
    "            # printing current value of the objective function at each iteration.\n",
    "            if t == epochs:\n",
    "                epochs = epochs + 10000\n",
    "                \n",
    "                print(f'Objective function at {t}:{sum_loss/t + (Lambda/2)*((LA.norm(self.w))**2)}')\n",
    "                   \n",
    "                           \n",
    "            \n",
    "        \n",
    "\n",
    "##### The following part is for the optional task.\n",
    "\n",
    "### Sparse and dense vectors don't collaborate very well in NumPy/SciPy.\n",
    "### Here are two utility functions that help us carry out some vector\n",
    "### operations that we'll need.\n",
    "\n",
    "def add_sparse_to_dense(x, w, factor):\n",
    "    \"\"\"\n",
    "    Adds a sparse vector x, scaled by some factor, to a dense vector.\n",
    "    This can be seen as the equivalent of w += factor * x when x is a dense\n",
    "    vector.\n",
    "    \"\"\"\n",
    "    w[x.indices] += factor * x.data\n",
    "\n",
    "def sparse_dense_dot(x, w):\n",
    "    \"\"\"\n",
    "    Computes the dot product between a sparse vector x and a dense vector w.\n",
    "    \"\"\"\n",
    "    return np.dot(w[x.indices], x.data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SparsePegasos(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the pegasos learning algorithm,\n",
    "    assuming that the input feature matrix X is sparse.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the pegasos learning algorithm.\n",
    "\n",
    "        Note that this will only work if X is a sparse matrix, such as the\n",
    "        output of a scikit-learn vectorizer.\n",
    "        \"\"\"\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "\n",
    "        # Iteration through sparse matrices can be a bit slow, so we first\n",
    "        # prepare this list to speed up iteration.\n",
    "        XY = list(zip(X, Ye))\n",
    "        \n",
    "        # number of iterations\n",
    "        T = 100000\n",
    "        \n",
    "        # Lambda\n",
    "        Lambda = 1/T\n",
    "        \n",
    "        for t in range(1,T+1):\n",
    "            i = np.random.randint(1,len(XY))\n",
    "            \n",
    "            #learning rate\n",
    "            nu = 1/(Lambda*t)\n",
    "\n",
    "            x = XY[i][0]\n",
    "            y = XY[i][1]\n",
    "           \n",
    "            score = sparse_dense_dot(x,self.w)\n",
    "            \n",
    "            if y*score < 1:\n",
    "                self.w = (1-nu*Lambda)*self.w\n",
    "                add_sparse_to_dense(x,self.w,nu*y)\n",
    "            else:\n",
    "                self.w = (1-nu*Lambda)*self.w\n",
    "                \n",
    "\n",
    "class SparsePegasos_scale(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the pegasos learning algorithm,\n",
    "    assuming that the input feature matrix X is sparse. \n",
    "    \n",
    "    In this implementation we use a scaling trick to speed up the process.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=20):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "\n",
    "        Note that this will only work if X is a sparse matrix, such as the\n",
    "        output of a scikit-learn vectorizer.\n",
    "        \"\"\"\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "\n",
    "        # Iteration through sparse matrices can be a bit slow, so we first\n",
    "        # prepare this list to speed up iteration.\n",
    "        XY = list(zip(X, Ye))\n",
    "        \n",
    "        #print(len(XY)) \n",
    "        T = 100000\n",
    "        \n",
    "        # Lambda\n",
    "        Lambda = 1/T\n",
    "        \n",
    "        a = 1 \n",
    "        \n",
    "        for t in range(1,T+1):\n",
    "            i = np.random.randint(1,len(XY))\n",
    "            \n",
    "            #learning rate\n",
    "            nu = 1/(Lambda*t)\n",
    "\n",
    "            x = XY[i][0]\n",
    "            y = XY[i][1]\n",
    "            \n",
    "            a = (1-nu*Lambda)*a\n",
    "           \n",
    "            score = sparse_dense_dot(x,self.w)*a\n",
    "            \n",
    "            if y*score < 1:\n",
    "                \n",
    "                add_sparse_to_dense(x,self.w,(nu*y)/a)       \n",
    "                \n",
    "            else:\n",
    "                \n",
    "                self.w = self.w\n",
    "          \n",
    "        self.w = a*self.w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d85269",
   "metadata": {},
   "source": [
    "### Code to run the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbaef6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to run the code the data needs to be in the same directory as the file. \n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# This function reads the corpus, returns a list of documents, and a list\n",
    "# of their corresponding polarity labels. \n",
    "def read_data(corpus_file):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(corpus_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            _, y, _, x = line.split(maxsplit=3)\n",
    "            X.append(x.strip())\n",
    "            Y.append(y)\n",
    "    return X, Y\n",
    "\n",
    "def run_pegasos():\n",
    "    \n",
    "    # Read all the documents.\n",
    "    X, Y = read_data('data/all_sentiment_shuffled.txt')\n",
    "    \n",
    "    # Split into training and test parts.\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "    \n",
    "    # Set up the preprocessing steps and the classifier.\n",
    "    pipeline = make_pipeline(\n",
    "        TfidfVectorizer(),\n",
    "        SelectKBest(k=1000),\n",
    "        Normalizer(),\n",
    "\n",
    "        # Choose wich classifier to use\n",
    "        Pegasos()\n",
    "    )\n",
    "\n",
    "    # Train the classifier.\n",
    "    t0 = time.time()\n",
    "    pipeline.fit(Xtrain, Ytrain)\n",
    "    t1 = time.time()\n",
    "    print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "    # Evaluate on the test set.\n",
    "    Yguess = pipeline.predict(Xtest)\n",
    "    print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))\n",
    "    \n",
    "def run_pegasos_lr():\n",
    "    \n",
    "    # Read all the documents.\n",
    "    X, Y = read_data('data/all_sentiment_shuffled.txt')\n",
    "    \n",
    "    # Split into training and test parts.\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "    \n",
    "    # Set up the preprocessing steps and the classifier.\n",
    "    pipeline = make_pipeline(\n",
    "        TfidfVectorizer(),\n",
    "        SelectKBest(k=1000),\n",
    "        Normalizer(),\n",
    "\n",
    "        # Choose wich classifier to use\n",
    "        Pegasos_LR()\n",
    "    )\n",
    "\n",
    "    # Train the classifier.\n",
    "    t0 = time.time()\n",
    "    pipeline.fit(Xtrain, Ytrain)\n",
    "    t1 = time.time()\n",
    "    print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "    # Evaluate on the test set.\n",
    "    Yguess = pipeline.predict(Xtest)\n",
    "    print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))\n",
    "\n",
    "def run_pegasos_BLAS():\n",
    "    \n",
    "    # Read all the documents.\n",
    "    X, Y = read_data('data/all_sentiment_shuffled.txt')\n",
    "    \n",
    "    # Split into training and test parts.\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "    \n",
    "    # Set up the preprocessing steps and the classifier.\n",
    "    pipeline = make_pipeline(\n",
    "        TfidfVectorizer(),\n",
    "        SelectKBest(k=1000),\n",
    "        Normalizer(),\n",
    "\n",
    "        # Choose wich classifier to use\n",
    "        Pegasos_BLAS()\n",
    "    )\n",
    "\n",
    "    # Train the classifier.\n",
    "    t0 = time.time()\n",
    "    pipeline.fit(Xtrain, Ytrain)\n",
    "    t1 = time.time()\n",
    "    print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "    # Evaluate on the test set.\n",
    "    Yguess = pipeline.predict(Xtest)\n",
    "    print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))\n",
    "\n",
    "\n",
    "    \n",
    "def run_pegasos_nosparse():\n",
    "    \n",
    "    # Read all the documents.\n",
    "    X, Y = read_data('data/all_sentiment_shuffled.txt')\n",
    "    \n",
    "    # Split into training and test parts.\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "    \n",
    "    # Set up the preprocessing steps and the classifier.\n",
    "    pipeline = make_pipeline(\n",
    "        TfidfVectorizer(ngram_range = (1,2)),\n",
    "        #SelectKBest(k=1000),\n",
    "        Normalizer(),\n",
    "\n",
    "        # Choose wich classifier to use\n",
    "        Pegasos()\n",
    "    )\n",
    "\n",
    "    # Train the classifier.\n",
    "    t0 = time.time()\n",
    "    pipeline.fit(Xtrain, Ytrain)\n",
    "    t1 = time.time()\n",
    "    print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "    # Evaluate on the test set.\n",
    "    Yguess = pipeline.predict(Xtest)\n",
    "    print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))\n",
    " \n",
    "\n",
    "def run_sparse_pegasos():\n",
    "    \n",
    "    \n",
    "    # Read all the documents.\n",
    "    X, Y = read_data('data/all_sentiment_shuffled.txt')\n",
    "    \n",
    "    # Split into training and test parts.\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "    \n",
    "    # Set up the preprocessing steps and the classifier.\n",
    "    pipeline = make_pipeline(\n",
    "        TfidfVectorizer(ngram_range = (1,2)),\n",
    "        #SelectKBest(k=1000),\n",
    "        Normalizer(),\n",
    "\n",
    "        # Choose wich classifier to use\n",
    "        SparsePegasos()\n",
    "    )\n",
    "\n",
    "    # Train the classifier.\n",
    "    t0 = time.time()\n",
    "    pipeline.fit(Xtrain, Ytrain)\n",
    "    t1 = time.time()\n",
    "    print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "    # Evaluate on the test set.\n",
    "    Yguess = pipeline.predict(Xtest)\n",
    "    print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "def run_scale_pegasos():\n",
    "    \n",
    "    # Read all the documents.\n",
    "    X, Y = read_data('data/all_sentiment_shuffled.txt')\n",
    "    \n",
    "    # Split into training and test parts.\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "    \n",
    "    # Set up the preprocessing steps and the classifier.\n",
    "    pipeline = make_pipeline(\n",
    "        TfidfVectorizer(ngram_range = (1,2)),\n",
    "        #SelectKBest(k=1000),\n",
    "        Normalizer(),\n",
    "\n",
    "        # Choose wich classifier to use\n",
    "        SparsePegasos_scale()\n",
    "    )\n",
    "\n",
    "    # Train the classifier.\n",
    "    t0 = time.time()\n",
    "    pipeline.fit(Xtrain, Ytrain)\n",
    "    t1 = time.time()\n",
    "    print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "    # Evaluate on the test set.\n",
    "    Yguess = pipeline.predict(Xtest)\n",
    "    print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3f3387",
   "metadata": {},
   "source": [
    "## MAIN:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a046a8ef",
   "metadata": {},
   "source": [
    "### Question 1: Implementing the SVC Pegasos algorithm\n",
    "\n",
    "We have chosen to follow the procedure in the paper by picking a fixed number T of randomly selected pairs to iterate through. We tried a few different values, and eventually chose T = 100 000 since it produced a fairly high accuracy and at a pretty good speed. Also, choosing a number higher than 100 000 did not lead to an increase in accuracy, but only an increase in time.\n",
    "\n",
    "From this, Lambda was set to 1/T.\n",
    "\n",
    "The code for the SVC pegasos classifier can be found in pegasos.py class Pegasos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2cc1d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 3.22 sec.\n",
      "Accuracy: 0.8212.\n"
     ]
    }
   ],
   "source": [
    "## Implementing the SVC pegasos algorithm\n",
    "\n",
    "run_pegasos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bbc530",
   "metadata": {},
   "source": [
    "### Question 2: Implementing the LR Pegasos algorithm\n",
    "\n",
    "We use the same number of iterations T and Lmabda as in the SVC case. \n",
    "\n",
    "The code for the LR pegasos classifier can be found in pegasos.py class Pegasos_LR.\n",
    "When running the code the Pegasos_LR class will also output the value of the objective function for every 10 000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c56c69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r7/wpkd3zdd0k3dcqgljnvbh6540000gn/T/ipykernel_60374/2933003409.py:215: RuntimeWarning: overflow encountered in exp\n",
      "  loss = -(y*x)/(1+ np.exp(y*score))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective function at 10000:0.26528079722833814\n",
      "Objective function at 20000:0.10519180105298849\n",
      "Objective function at 30000:0.0706904483678964\n",
      "Objective function at 40000:0.05796623758457827\n",
      "Objective function at 50000:0.05134246491714277\n",
      "Objective function at 60000:0.04739245635404251\n",
      "Objective function at 70000:0.044650843311007035\n",
      "Objective function at 80000:0.04375890881184911\n",
      "Objective function at 90000:0.042640701222554644\n",
      "Objective function at 100000:0.04177806210805282\n",
      "Training time: 5.73 sec.\n",
      "Accuracy: 0.8254.\n"
     ]
    }
   ],
   "source": [
    "## Implementing the LR pegasos algorithm\n",
    "\n",
    "run_pegasos_lr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08f13f0",
   "metadata": {},
   "source": [
    "Using the log-loss function raises the accuarcy slightly (0.8254) but with an increased training time (5.73 seconds). This might be due to the fact that the log-loss uses more computations than the hinge-loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8fb9e9",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "### Bonus task 1: Making your code more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6cd945",
   "metadata": {},
   "source": [
    "#### a) Faster linear algebra operations\n",
    "\n",
    "The code for the SVC pegasos classifier using BLAS functions can be found in pegasos.py class Pegasos_BLAS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53390e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 2.67 sec.\n",
      "Accuracy: 0.8196.\n"
     ]
    }
   ],
   "source": [
    "## Implementing SVC pegasos algorithm using BLAS functions\n",
    "\n",
    "run_pegasos_BLAS()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a920a1c2",
   "metadata": {},
   "source": [
    "Using the BLAS function helped speed up the linear algebra operations.\n",
    "In question 1, we got a training time of 3.22 seconds and an accuracy of 0.8212. Using BLAS functions we got a training time of 2.67 seconds and a similar accuracy of 0.8196.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62e83af",
   "metadata": {},
   "source": [
    "#### b) Using sparse vectors\n",
    "\n",
    "We start by running the original SVC pegasos from question 1 but this time without using SelectKbest and changing the TFIDF vectorizer ngram range to (1,2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39a69667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 465.57 sec.\n",
      "Accuracy: 0.8741.\n"
     ]
    }
   ],
   "source": [
    "## Implementing the SVC pegasos algorithm without Kbest and ngram_range = (1,2)\n",
    " \n",
    "run_pegasos_nosparse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124a21f5",
   "metadata": {},
   "source": [
    "The accuracy has increased a bit, which is expected since we are utlizing a larger set of features. However, the training time has increased significantly!\n",
    "\n",
    "Next step is to try the sparse version of SVC pegasos:\n",
    "\n",
    "The code for the sparse SVC pegasos classifier can be found in pegasos.py class SparsePegasos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acab230c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 143.03 sec.\n",
      "Accuracy: 0.8724.\n"
     ]
    }
   ],
   "source": [
    "## Implementing the SVC pegasos algorithm using sparse vectors\n",
    "## Remove SelectKBest \n",
    "## In the TFIDF-vectorizer, change ngram range to (1,2)\n",
    "\n",
    "run_sparse_pegasos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63103fd5",
   "metadata": {},
   "source": [
    "By using sparse vectors we managed to decrease the training time from 465.57 seconds to 143.03 seconds while maintaining the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc770e29",
   "metadata": {},
   "source": [
    "#### c) Speeding up the scaling operation\n",
    "\n",
    "The code for the sparse SVC pegasos classifier with the scaling trick can be found in pegasos.py class SparsePegasos_scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4392b1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 7.78 sec.\n",
      "Accuracy: 0.8758.\n"
     ]
    }
   ],
   "source": [
    "## Implementing the SVC pegasos algorithm using sparse vectors and scaling\n",
    "## Remove SelectKBest \n",
    "## In the TFIDF-vectorizer, change ngram range to (1,2)\n",
    "\n",
    "run_scale_pegasos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d972a7",
   "metadata": {},
   "source": [
    "With the sclaing trick the training time was dramatically reduced, from 143.03 seconds to 7.78 seconds, again the accuracy is maintained at approximately the same level (it varies because of the randomness in sampling at each iteration T)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
